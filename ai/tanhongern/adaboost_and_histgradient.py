# -*- coding: utf-8 -*-
"""AdaBoost and HistGradient.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n1z5KJ0x7M4PqiaVDtUW2UxUnRKtI5dY

# Step 1: Gathering Data

# This time you need to add comments
There are questions above the code. You can take them as a guidance of what you can write in the comments.
If possible, please replace "# add comments here" to your own comments

What are we importing and why?
"""

# Commented out IPython magic to ensure Python compatibility.
# add comments here

import numpy as np                # add comments here
import matplotlib.pyplot as plt   # add comments here
import seaborn as sns             # add comments here
import pandas as pd               # add comments here
# %matplotlib inline

"""why are we giving column features?"""

# add comments here

column_features = ['Forehead_width','Cheeks_width','Jawline_length','Face_length','Face_Type']

"""what does "df" do?"""

# add comments here

df = pd.read_csv('/content/faces_data.csv', names=column_features)

"""# Step 2: Preparing Data

*   why area we looking at this?
*   what can we learn from our data when we look at this?
"""

# add comments here

df.head(5)

"""* why are we getting this results?
* what does this result mean?
* How useful is it to look at the mean values of each column for this dataset?
"""

# add comments here

df.describe()

"""- what are the types of faces can you change here?
- why are we doing this?
- write down the result for each of the targets

You can change to the following:
- "square"
- "diamond"
- "triangle"
- "oblong"
- "oval"
- "heart"
"""

# add comments here
face_type_count = len(df[df["Face_Type"]=="square"])

print(face_type_count)

"""- why are we making this plot?
- what can we learn from this plot?
- which datapoints are overlapping?
- which graph show us a seperation of the classes?
"""

# add comments here

sns.pairplot(df, hue='Face_Type')

"""- what does "[:]" mean?
- why are we saving these values in "X" and "Y"?
"""

# add comments here

data = df.values    # add comments here
X = data[:,0:4]     # add comments here
Y = data[:,4]       # add comments here

"""why do we need to use .reshape()?"""

# add comments here

Y_Data = np.array([np.average(X[:, i][Y==j].astype('float32')) for i in range (X.shape[1]) for j in (np.unique(Y))])    # add comments here
Y_Data_reshaped = Y_Data.reshape(4,6)
Y_Data_reshaped = np.swapaxes(Y_Data_reshaped, 0, 1)
X_axis = np.arange(len(column_features)-1)
width = 0.1

"""- what are we plotting and why?
- what can we learn from this data?
- which features are similar?
"""

# add comments here

plt.bar(X_axis, Y_Data_reshaped[0], width, label = 'square')
plt.bar(X_axis+width, Y_Data_reshaped[1], width, label = 'diamond')
plt.bar(X_axis+width*2, Y_Data_reshaped[2], width, label = 'triangle')
plt.bar(X_axis+width*3, Y_Data_reshaped[2], width, label = 'oblong')
plt.bar(X_axis+width*4, Y_Data_reshaped[2], width, label = 'oval')
plt.bar(X_axis+width*5, Y_Data_reshaped[2], width, label = 'heart')
plt.xticks(X_axis, column_features[:4])
plt.xlabel("Features")
plt.ylabel("Values")
plt.legend(bbox_to_anchor=(1.3,1))
plt.show()

"""# Step 3: Choosing a Model

- what does "train_test_split" do and why are we using it?
"""

# add comments here

from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1)   # add comments here

"""- why are we using this model and what is another model we could use?:"""

# add comments here
from sklearn.svm import SVC
svn = SVC()
svn.fit(X_train, y_train)

# you can consider a Decision Tree Classifier
# from sklearn.tree import DecisionTreeClassifier

# dt = DecisionTreeClassifier()                     # save decision tree
# if you're using decision tree, you'll just need to change the code below from "svn" to "dt"
clf = AdaBoostClassifier()
clf.fit(X_train, y_train)

"""- what does ".predict()" do?"""

# add comments here

predictions = clf.predict(X_test)

# what does "accuracy_score" does and what does it mean?
# add comments here

from sklearn.metrics import accuracy_score
accuracy_score(y_test, predictions)

# add comments here

from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))

X_new = np.array([[13.73, 12.36, 17.24,18],[11, 11, 13,18],[15, 15, 17.24,15],[12.92,12.43, 14.88,20.88],[11.81, 12.93, 12.82,17.47]])

prediction = svn.predict(X_new)
print("Prediction of Face type: {}".format(prediction))

from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)

from sklearn.svm import SVC
svn = SVC()
svn.fit(X_train, y_train)
clf = HistGradientBoostingClassifier()
clf.fit(X_train, y_train)

predictions = clf.predict(X_test)


from sklearn.metrics import accuracy_score
accuracy_score(y_test, predictions)

from sklearn.metrics import classification_report
print(classification_report(y_test, predictions))

X_new = np.array([[13.73, 12.36, 17.24,18],[11, 11, 13,18],[15, 15, 17.24,15],[12.92,12.43, 14.88,20.88],[11.81, 12.93, 12.82,17.47]])

prediction = svn.predict(X_new)
print("Prediction of Face type: {}".format(prediction))